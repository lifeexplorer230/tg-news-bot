"""
FIX-DUPLICATE-3: –¢–µ—Å—Ç—ã –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ encoding

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —á—Ç–æ:
1. –¢–µ–∫—Å—Ç –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç—Å—è –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º embeddings
2. –£–¥–∞–ª—è—é—Ç—Å—è –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
3. URL –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –º–∞—Ä–∫–µ—Ä (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
4. –≠–º–æ–¥–∑–∏ —É–¥–∞–ª—è—é—Ç—Å—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ –º–µ–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É —Ç–µ–∫—Å—Ç–∞
"""

import numpy as np
import pytest

from services.embeddings import EmbeddingService, normalize_text_for_embedding


class TestTextNormalization:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""

    def test_normalize_removes_extra_whitespace(self):
        """–£–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫"""
        text = "Ozon   —Å–Ω–∏–∑–∏–ª\n\n–∫–æ–º–∏—Å—Å–∏—é  –¥–ª—è\t\t–ø—Ä–æ–¥–∞–≤—Ü–æ–≤"
        normalized = normalize_text_for_embedding(text)

        # –î–æ–ª–∂–Ω—ã –æ—Å—Ç–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        assert "  " not in normalized
        assert "\n" not in normalized
        assert "\t" not in normalized
        assert normalized == "Ozon —Å–Ω–∏–∑–∏–ª –∫–æ–º–∏—Å—Å–∏—é –¥–ª—è –ø—Ä–æ–¥–∞–≤—Ü–æ–≤"

    def test_normalize_removes_leading_trailing_spaces(self):
        """–£–¥–∞–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ"""
        text = "   –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏   \n\n"
        normalized = normalize_text_for_embedding(text)

        assert normalized == "–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏"
        assert not normalized.startswith(" ")
        assert not normalized.endswith(" ")

    def test_normalize_replaces_urls_with_marker(self):
        """–ó–∞–º–µ–Ω—è–µ—Ç URL –Ω–∞ –º–∞—Ä–∫–µ—Ä [URL]"""
        text = "–°–º–æ—Ç—Ä–∏ https://example.com/news –∏ http://test.ru —Ç—É—Ç"
        normalized = normalize_text_for_embedding(text, remove_urls=True)

        # URL –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ –º–∞—Ä–∫–µ—Ä
        assert "https://example.com/news" not in normalized
        assert "http://test.ru" not in normalized
        assert "[URL]" in normalized
        assert normalized == "–°–º–æ—Ç—Ä–∏ [URL] –∏ [URL] —Ç—É—Ç"

    def test_normalize_keeps_urls_if_disabled(self):
        """–ù–µ —É–¥–∞–ª—è–µ—Ç URL –µ—Å–ª–∏ remove_urls=False"""
        text = "–°—Å—ã–ª–∫–∞: https://example.com"
        normalized = normalize_text_for_embedding(text, remove_urls=False)

        assert "https://example.com" in normalized

    def test_normalize_removes_emoji(self):
        """–£–¥–∞–ª—è–µ—Ç —ç–º–æ–¥–∑–∏"""
        text = "–û—Ç–ª–∏—á–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å! üî•üéâ Ozon —Å–Ω–∏–∑–∏–ª –∫–æ–º–∏—Å—Å–∏—é üëç"
        normalized = normalize_text_for_embedding(text, remove_emoji=True)

        # –≠–º–æ–¥–∑–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–¥–∞–ª–µ–Ω—ã
        assert "üî•" not in normalized
        assert "üéâ" not in normalized
        assert "üëç" not in normalized
        assert normalized == "–û—Ç–ª–∏—á–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å! Ozon —Å–Ω–∏–∑–∏–ª –∫–æ–º–∏—Å—Å–∏—é"

    def test_normalize_keeps_emoji_if_disabled(self):
        """–ù–µ —É–¥–∞–ª—è–µ—Ç —ç–º–æ–¥–∑–∏ –µ—Å–ª–∏ remove_emoji=False"""
        text = "–ù–æ–≤–æ—Å—Ç—å üî•"
        normalized = normalize_text_for_embedding(text, remove_emoji=False)

        assert "üî•" in normalized

    def test_normalize_handles_empty_text(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç"""
        normalized = normalize_text_for_embedding("")
        assert normalized == ""

        normalized = normalize_text_for_embedding("   \n\n  ")
        assert normalized == ""

    def test_normalize_preserves_cyrillic(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É"""
        text = "–†–æ—Å—Å–∏–π—Å–∫–∏–π –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å Wildberries"
        normalized = normalize_text_for_embedding(text)

        assert "–†–æ—Å—Å–∏–π—Å–∫–∏–π" in normalized
        assert "–º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å" in normalized
        assert "Wildberries" in normalized

    def test_normalize_preserves_numbers(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∏—Å–ª–∞"""
        text = "–ö–æ–º–∏—Å—Å–∏—è —Å–Ω–∏–∂–µ–Ω–∞ –Ω–∞ 2% —Å 15 –¥–æ 13 —Ä—É–±–ª–µ–π"
        normalized = normalize_text_for_embedding(text)

        assert "2%" in normalized or "2" in normalized
        assert "15" in normalized
        assert "13" in normalized

    def test_normalize_handles_multiple_urls(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ URL"""
        text = "–ß–∏—Ç–∞–π –Ω–∞ https://site1.com –∏ https://site2.ru/path/to/page"
        normalized = normalize_text_for_embedding(text, remove_urls=True)

        count_url_markers = normalized.count("[URL]")
        assert count_url_markers == 2


class TestEmbeddingServiceNormalization:
    """–¢–µ—Å—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ EmbeddingService"""

    @pytest.fixture
    def embedding_service(self):
        """EmbeddingService —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        return EmbeddingService(
            model_name="paraphrase-multilingual-MiniLM-L12-v2",
            enable_text_normalization=True,
        )

    @pytest.fixture
    def embedding_service_no_norm(self):
        """EmbeddingService –±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)"""
        return EmbeddingService(
            model_name="paraphrase-multilingual-MiniLM-L12-v2",
            enable_text_normalization=False,
        )

    def test_similar_texts_with_different_whitespace_produce_same_embeddings(
        self, embedding_service
    ):
        """
        –¢–µ–∫—Å—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏/–ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ –¥–æ–ª–∂–Ω—ã –¥–∞–≤–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ embeddings

        –≠—Ç–æ –≥–ª–∞–≤–Ω–∞—è —Ü–µ–ª—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ - —É—Å—Ç—Ä–∞–Ω–∏—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–ª–∏—á–∏—è
        """
        text1 = "Ozon —Å–Ω–∏–∑–∏–ª –∫–æ–º–∏—Å—Å–∏—é –¥–ª—è –ø—Ä–æ–¥–∞–≤—Ü–æ–≤ –Ω–∞ 2%"
        text2 = "Ozon   —Å–Ω–∏–∑–∏–ª\n\n–∫–æ–º–∏—Å—Å–∏—é  –¥–ª—è\t\t–ø—Ä–æ–¥–∞–≤—Ü–æ–≤   –Ω–∞  2%"

        embedding1 = embedding_service.encode(text1)
        embedding2 = embedding_service.encode(text2)

        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–º (–±–ª–∏–∑–∫–æ –∫ 1.0)
        similarity = embedding_service.cosine_similarity(embedding1, embedding2)

        assert similarity > 0.99, (
            f"–¢–µ–∫—Å—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏ –¥–æ–ª–∂–Ω—ã –¥–∞–≤–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ embeddings, "
            f"–ø–æ–ª—É—á–µ–Ω–æ similarity={similarity:.3f}"
        )

    def test_texts_with_urls_more_similar_after_normalization(
        self, embedding_service, embedding_service_no_norm
    ):
        """
        –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è URL —Ç–µ–∫—Å—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–º–∏

        –°—Ü–µ–Ω–∞—Ä–∏–π: –æ–¥–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        """
        text1 = "Ozon —Å–Ω–∏–∑–∏–ª –∫–æ–º–∏—Å—Å–∏—é https://source1.com"
        text2 = "Ozon —Å–Ω–∏–∑–∏–ª –∫–æ–º–∏—Å—Å–∏—é https://source2.com"

        # –ë–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ - embeddings –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –∏–∑-–∑–∞ —Ä–∞–∑–Ω—ã—Ö URL
        emb1_no_norm = embedding_service_no_norm.encode(text1)
        emb2_no_norm = embedding_service_no_norm.encode(text2)
        similarity_no_norm = embedding_service_no_norm.cosine_similarity(
            emb1_no_norm, emb2_no_norm
        )

        # –° –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π - URL –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ [URL], embeddings –∏–¥–µ–Ω—Ç–∏—á–Ω—ã
        emb1_norm = embedding_service.encode(text1)
        emb2_norm = embedding_service.encode(text2)
        similarity_norm = embedding_service.cosine_similarity(emb1_norm, emb2_norm)

        # –° –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π similarity –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤—ã—à–µ
        assert similarity_norm > similarity_no_norm, (
            f"–ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–º–∏. "
            f"–ë–µ–∑ –Ω–æ—Ä–º: {similarity_no_norm:.3f}, —Å –Ω–æ—Ä–º: {similarity_norm:.3f}"
        )
        assert similarity_norm > 0.95, (
            f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è URL —Ç–µ–∫—Å—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ—á—Ç–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, "
            f"–ø–æ–ª—É—á–µ–Ω–æ similarity={similarity_norm:.3f}"
        )

    def test_normalization_does_not_break_semantic_meaning(self, embedding_service):
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ –¥–æ–ª–∂–Ω–∞ –ª–æ–º–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏–∫—É —Ç–µ–∫—Å—Ç–∞

        –†–∞–∑–Ω—ã–µ –ø–æ —Å–º—ã—Å–ª—É —Ç–µ–∫—Å—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è —Ä–∞–∑–Ω—ã–º–∏
        """
        text1 = "Ozon —Å–Ω–∏–∑–∏–ª –∫–æ–º–∏—Å—Å–∏—é –¥–ª—è –ø—Ä–æ–¥–∞–≤—Ü–æ–≤"
        text2 = "Wildberries –ø–æ–≤—ã—Å–∏–ª –∫–æ–º–∏—Å—Å–∏—é –¥–ª—è –ø—Ä–æ–¥–∞–≤—Ü–æ–≤"

        embedding1 = embedding_service.encode(text1)
        embedding2 = embedding_service.encode(text2)

        similarity = embedding_service.cosine_similarity(embedding1, embedding2)

        # –¢–µ–∫—Å—Ç—ã —Ä–∞–∑–Ω—ã–µ –ø–æ —Å–º—ã—Å–ª—É (—Å–Ω–∏–∑–∏–ª vs –ø–æ–≤—ã—Å–∏–ª, —Ä–∞–∑–Ω—ã–µ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã)
        assert similarity < 0.85, (
            f"–†–∞–∑–Ω—ã–µ –ø–æ —Å–º—ã—Å–ª—É —Ç–µ–∫—Å—Ç—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –Ω–∏–∑–∫—É—é similarity, "
            f"–ø–æ–ª—É—á–µ–Ω–æ {similarity:.3f}"
        )


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
