"""–†–∞–±–æ—Ç–∞ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö SQLite"""

from __future__ import annotations

import io
import sqlite3
import threading
import time
from datetime import datetime, timedelta
from pathlib import Path

import numpy as np

from utils.logger import setup_logger
from utils.timezone import get_timezone, now_in_timezone, now_msk, now_utc, to_utc
from database.connection_pool import ConnectionPool

logger = setup_logger(__name__)


def retry_on_locked(func):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ –ë–î"""

    def wrapper(self, *args, **kwargs):
        max_retries = getattr(self, "_retry_max_attempts", 5)
        base_delay = getattr(self, "_retry_base_delay", 0.5)
        multiplier = getattr(self, "_retry_backoff_multiplier", 1.0)

        for attempt in range(max_retries):
            try:
                return func(self, *args, **kwargs)
            except sqlite3.OperationalError as e:
                if "locked" in str(e) and attempt < max_retries - 1:
                    delay = base_delay * (attempt + 1) * multiplier
                    logger.warning(
                        "–ë–î –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞, –ø–æ–ø—ã—Ç–∫–∞ %s/%s, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ %.2f c",
                        attempt + 1,
                        max_retries,
                        delay,
                    )
                    time.sleep(delay)
                    continue
                raise
        return None

    return wrapper


class Database:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å SQLite –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""

    def __init__(
        self,
        db_path: str,
        *,
        timeout: float = 30.0,
        busy_timeout_ms: int = 30000,
        retry_max_attempts: int = 5,
        retry_base_delay: float = 0.5,
        retry_backoff_multiplier: float = 1.0,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

        Args:
            db_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        """
        self.db_path = db_path
        self._timeout = float(timeout)
        self._busy_timeout_ms = int(busy_timeout_ms)
        self._retry_max_attempts = max(1, int(retry_max_attempts))
        self._retry_base_delay = max(0.0, float(retry_base_delay))
        self._retry_backoff_multiplier = max(0.0, float(retry_backoff_multiplier)) or 1.0
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)

        # Connection pool –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self._pool = ConnectionPool(
            db_path,
            max_connections=5,
            timeout=timeout,
            busy_timeout_ms=busy_timeout_ms
        )

        self._conn: sqlite3.Connection | None = None  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (—Ç–µ—Å—Ç—ã, reels)
        self._lock = threading.Lock()  # Thread safety –¥–ª—è write operations
        self._closed = False
        self.init_db()

    @property
    def conn(self) -> sqlite3.Connection:
        """Backward-compat: –ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ—Å—Ç–∞–º–∏ –∏ –≤–Ω–µ—à–Ω–∏–º –∫–æ–¥–æ–º (reels). –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç–æ–¥—ã
        Database –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—É–ª —á–µ—Ä–µ–∑ ``self._pool.get_connection()``.
        """
        if self._conn is None:
            self._conn = self._pool._create_connection()
        return self._conn

    @conn.setter
    def conn(self, value):
        self._conn = value

    def connect(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        return self.conn

    def get_connection(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–∑ –ø—É–ª–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –º–µ—Ç–æ–¥)"""
        return self._pool.get_connection()

    def init_db(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç"""
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()

            # –¢–∞–±–ª–∏—Ü–∞ –∫–∞–Ω–∞–ª–æ–≤
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS channels (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    username TEXT UNIQUE NOT NULL,
                    title TEXT,
                    is_active BOOLEAN DEFAULT 1,
                    added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # –¢–∞–±–ª–∏—Ü–∞ —Å—ã—Ä—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS raw_messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    channel_id INTEGER NOT NULL,
                    message_id INTEGER NOT NULL,
                    text TEXT NOT NULL,
                    date TIMESTAMP NOT NULL,
                    has_media BOOLEAN DEFAULT 0,
                    processed BOOLEAN DEFAULT 0,
                    is_duplicate BOOLEAN DEFAULT 0,
                    gemini_score INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (channel_id) REFERENCES channels(id),
                    UNIQUE(channel_id, message_id)
                )
            """
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ –ø–æ–ª–µ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            cursor.execute("PRAGMA table_info(raw_messages)")
            columns = [col[1] for col in cursor.fetchall()]
            if "rejection_reason" not in columns:
                cursor.execute("ALTER TABLE raw_messages ADD COLUMN rejection_reason TEXT")
                logger.info("–î–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ–ª–µ rejection_reason –≤ raw_messages")

            # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è raw_messages
            cursor.execute(
                """
                CREATE INDEX IF NOT EXISTS idx_processed
                ON raw_messages(processed, date)
            """
            )
            cursor.execute(
                """
                CREATE INDEX IF NOT EXISTS idx_date
                ON raw_messages(date)
            """
            )

            # –¢–∞–±–ª–∏—Ü–∞ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS published (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    text TEXT NOT NULL,
                    embedding BLOB,
                    source_message_id INTEGER,
                    source_channel_id INTEGER,
                    published_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (source_message_id) REFERENCES raw_messages(id)
                )
            """
            )

            cursor.execute(
                """
                CREATE INDEX IF NOT EXISTS idx_published_date
                ON published(published_at)
            """
            )

            # –ú–∏–≥—Ä–∞—Ü–∏—è: —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –¥–æ–±–∞–≤–ª—è–µ–º UNIQUE constraint –Ω–∞ source_message_id
            self._migrate_published_unique_constraint(cursor)

            # –¢–∞–±–ª–∏—Ü–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS config (
                    key TEXT PRIMARY KEY,
                    value TEXT
                )
            """
            )

            # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS channel_stats (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    channel_id INTEGER NOT NULL REFERENCES channels(id),
                    scanned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    participants_count INTEGER DEFAULT 0,
                    avg_message_views INTEGER DEFAULT 0,
                    description TEXT,
                    contact_info TEXT
                )
            """
            )
            cursor.execute(
                """
                CREATE INDEX IF NOT EXISTS idx_channel_stats_channel
                ON channel_stats(channel_id, scanned_at DESC)
            """
            )

            # –ú–∏–≥—Ä–∞—Ü–∏—è channels_meta: –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–ª—è –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
            self._migrate_channels_meta(cursor)

            logger.info(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {self.db_path}")

    def _migrate_published_unique_constraint(self, cursor: sqlite3.Cursor):
        """
        –ú–∏–≥—Ä–∞—Ü–∏—è: –¥–æ–±–∞–≤–∏—Ç—å UNIQUE constraint –Ω–∞ (source_message_id, source_channel_id)

        –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø—É–±–ª–∏–∫–∞—Ü–∏—é –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–≤–∞–∂–¥—ã.
        –ú–∏–≥—Ä–∞—Ü–∏—è –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–∞ - –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ.
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –∏–Ω–¥–µ–∫—Å
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='index' AND name='idx_published_source_unique'"
        )
        if cursor.fetchone():
            return

        logger.info("üîÑ –ó–∞–ø—É—Å–∫ –º–∏–≥—Ä–∞—Ü–∏–∏: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ UNIQUE constraint –Ω–∞ published")

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
        cursor.execute("""
            SELECT COUNT(*) FROM published
            WHERE id NOT IN (
                SELECT MAX(id) FROM published
                WHERE source_message_id IS NOT NULL
                GROUP BY source_message_id, source_channel_id
            )
            AND source_message_id IS NOT NULL
        """)
        duplicate_count = cursor.fetchone()[0]

        if duplicate_count > 0:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {duplicate_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ published, —É–¥–∞–ª—è–µ–º...")

            cursor.execute("""
                DELETE FROM published
                WHERE id NOT IN (
                    SELECT MAX(id) FROM published
                    WHERE source_message_id IS NOT NULL
                    GROUP BY source_message_id, source_channel_id
                )
                AND source_message_id IS NOT NULL
            """)
            logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {duplicate_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")

        cursor.execute("""
            CREATE UNIQUE INDEX IF NOT EXISTS idx_published_source_unique
            ON published(source_message_id, source_channel_id)
            WHERE source_message_id IS NOT NULL
        """)
        logger.info("‚úÖ –°–æ–∑–¥–∞–Ω UNIQUE –∏–Ω–¥–µ–∫—Å idx_published_source_unique")

    def _migrate_channels_meta(self, cursor: sqlite3.Cursor):
        """–î–æ–±–∞–≤–∏—Ç—å –ø–æ–ª—è description, contact_info, stats_updated_at –≤ channels_meta –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç."""
        cursor.execute("PRAGMA table_info(channels_meta)")
        existing = {row[1] for row in cursor.fetchall()}
        for col, definition in [
            ("description", "TEXT"),
            ("contact_info", "TEXT"),
            ("stats_updated_at", "TIMESTAMP"),
        ]:
            if col not in existing:
                cursor.execute(f"ALTER TABLE channels_meta ADD COLUMN {col} {definition}")
                logger.info("–î–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ–ª–µ %s –≤ channels_meta", col)

    # ====== –†–ê–ë–û–¢–ê –° –ö–ê–ù–ê–õ–ê–ú–ò ======

    @retry_on_locked
    def add_channel(self, username: str, title: str = "") -> int:
        """
        –î–æ–±–∞–≤–∏—Ç—å –∫–∞–Ω–∞–ª –≤ –±–∞–∑—É

        Args:
            username: Username –∫–∞–Ω–∞–ª–∞ (—Å @ –∏–ª–∏ –±–µ–∑)
            title: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞

        Returns:
            ID –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        """
        username = username.lstrip("@")
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()
            try:
                cursor.execute(
                    "INSERT INTO channels (username, title) VALUES (?, ?)", (username, title)
                )
                logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω –∫–∞–Ω–∞–ª: @{username}")
                return cursor.lastrowid
            except sqlite3.IntegrityError:
                cursor.execute("SELECT id FROM channels WHERE username = ?", (username,))
                return cursor.fetchone()[0]

    def get_channel_id(self, username: str) -> int | None:
        """–ü–æ–ª—É—á–∏—Ç—å ID –∫–∞–Ω–∞–ª–∞ –ø–æ username"""
        username = username.lstrip("@")
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id FROM channels WHERE username = ?", (username,))
            row = cursor.fetchone()
            return row[0] if row else None

    def get_active_channels(self) -> list[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤"""
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM channels WHERE is_active = 1")
            return [dict(row) for row in cursor.fetchall()]

    @retry_on_locked
    def update_channel_stats(
        self,
        channel_id: int,
        participants_count: int,
        avg_message_views: int,
        description: str,
        contact_info: str,
    ) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–∞: –≤ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫—É—é —Ç–∞–±–ª–∏—Ü—É –∏ –æ–±–Ω–æ–≤–∏—Ç—å channels_meta."""
        now = datetime.utcnow()
        with self._pool.get_connection() as conn:
            conn.execute(
                """INSERT INTO channel_stats
                   (channel_id, scanned_at, participants_count, avg_message_views, description, contact_info)
                   VALUES (?, ?, ?, ?, ?, ?)""",
                (channel_id, now, participants_count, avg_message_views, description, contact_info),
            )
            conn.execute(
                """UPDATE channels_meta
                   SET subscribers=?, avg_views=?, description=?, contact_info=?, stats_updated_at=?
                   WHERE channel_id=?""",
                (participants_count, avg_message_views, description, contact_info, now, channel_id),
            )

    # ====== –†–ê–ë–û–¢–ê –° –°–û–û–ë–©–ï–ù–ò–Ø–ú–ò ======

    @retry_on_locked
    def save_message(
        self, channel_id: int, message_id: int, text: str, date: datetime, has_media: bool = False
    ) -> int | None:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ –∫–∞–Ω–∞–ª–∞

        Args:
            channel_id: ID –∫–∞–Ω–∞–ª–∞
            message_id: ID —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram
            text: –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
            date: –î–∞—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
            has_media: –ï—Å—Ç—å –ª–∏ –º–µ–¥–∏–∞

        Returns:
            ID –∑–∞–ø–∏—Å–∏ –∏–ª–∏ None –µ—Å–ª–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        """
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()
            try:
                cursor.execute(
                    """
                    INSERT INTO raw_messages
                    (channel_id, message_id, text, date, has_media)
                    VALUES (?, ?, ?, ?, ?)
                """,
                    (channel_id, message_id, text, date, has_media),
                )
                return cursor.lastrowid
            except sqlite3.IntegrityError:
                return None

    def get_unprocessed_messages(self, hours: int = 24) -> list[dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤

        Args:
            hours: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –Ω–∞–∑–∞–¥

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º UTC –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, —Ç.–∫. message.date —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ UTC
            cutoff_time = now_utc() - timedelta(hours=hours)

            cursor.execute(
                """
                SELECT m.*, c.username as channel_username
                FROM raw_messages m
                JOIN channels c ON m.channel_id = c.id
                WHERE m.processed = 0
                  AND m.date > ?
                ORDER BY m.date DESC
            """,
                (cutoff_time,),
            )

            return [dict(row) for row in cursor.fetchall()]

    @retry_on_locked
    def mark_as_processed(
        self,
        message_id: int,
        is_duplicate: bool = False,
        gemini_score: int | None = None,
        rejection_reason: str | None = None,
    ):
        """
        –ü–æ–º–µ—Ç–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ

        Args:
            message_id: ID —Å–æ–æ–±—â–µ–Ω–∏—è
            is_duplicate: –Ø–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–º
            gemini_score: –û—Ü–µ–Ω–∫–∞ Gemini (–¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–π)
            rejection_reason: –ü—Ä–∏—á–∏–Ω–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ –Ω–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ)
        """
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                UPDATE raw_messages
                SET processed = 1,
                    is_duplicate = ?,
                    gemini_score = ?,
                    rejection_reason = ?
                WHERE id = ?
            """,
                (is_duplicate, gemini_score, rejection_reason, message_id),
            )

    @retry_on_locked
    def mark_as_processed_batch(self, updates: list[dict]):
        """
        –ë–∞—Ç—á-–ø–æ–º–µ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∑–∞ –æ–¥–Ω—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é (Sprint 6.1)

        Args:
            updates: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–æ–ª—è–º–∏:
                - message_id: int (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
                - is_duplicate: bool (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
                - gemini_score: int | None (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None)
                - rejection_reason: str | None (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None)
        """
        if not updates:
            return

        with self._pool.get_connection() as conn:
            cursor = conn.cursor()

            batch_data = [
                (
                    update.get('is_duplicate', False),
                    update.get('gemini_score'),
                    update.get('rejection_reason'),
                    update['message_id']
                )
                for update in updates
            ]

            # –Ø–≤–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –¥–ª—è –∞—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç–∏ –±–∞—Ç—á–∞
            conn.execute("BEGIN")
            try:
                cursor.executemany(
                    """
                    UPDATE raw_messages
                    SET processed = 1,
                        is_duplicate = ?,
                        gemini_score = ?,
                        rejection_reason = ?
                    WHERE id = ?
                """,
                    batch_data
                )
                conn.execute("COMMIT")
            except Exception:
                conn.execute("ROLLBACK")
                raise

            logger.debug(f"Batch processed {len(updates)} messages")

    # ====== –†–ê–ë–û–¢–ê –° –û–ü–£–ë–õ–ò–ö–û–í–ê–ù–ù–´–ú–ò –ü–û–°–¢–ê–ú–ò ======

    @retry_on_locked
    def save_published(
        self, text: str, embedding: np.ndarray, source_message_id: int, source_channel_id: int
    ) -> int:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Å—Ç

        Args:
            text: –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞
            embedding: Embedding –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            source_message_id: ID –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            source_channel_id: ID –∫–∞–Ω–∞–ª–∞-–∏—Å—Ç–æ—á–Ω–∏–∫–∞

        Returns:
            ID –∑–∞–ø–∏—Å–∏ –∏–ª–∏ -1 –µ—Å–ª–∏ –∑–∞–ø–∏—Å—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (–¥—É–±–ª–∏–∫–∞—Ç)
        """
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()
            buffer = io.BytesIO()
            np.save(buffer, embedding, allow_pickle=False)
            embedding_bytes = buffer.getvalue()

            cursor.execute(
                """
                INSERT OR IGNORE INTO published
                (text, embedding, source_message_id, source_channel_id)
                VALUES (?, ?, ?, ?)
            """,
                (text, embedding_bytes, source_message_id, source_channel_id),
            )

            if cursor.rowcount == 0:
                logger.warning(
                    f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç –ø—Ä–æ–ø—É—â–µ–Ω –≤ save_published: "
                    f"source_message_id={source_message_id}, source_channel_id={source_channel_id}"
                )
                return -1
            return cursor.lastrowid


    def get_recently_published_texts(self, days: int = 7, limit: int = 30) -> list[dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –Ω–µ–¥–∞–≤–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏.

        Args:
            days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å text (–æ–±—Ä–µ–∑–∞–Ω –¥–æ 150 —Å–∏–º–≤–æ–ª–æ–≤) –∏ published_at
        """
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()
            cutoff_time = now_utc() - timedelta(days=days)
            cursor.execute(
                """
                SELECT text, published_at FROM published
                WHERE published_at > ?
                ORDER BY published_at DESC
                LIMIT ?
                """,
                (cutoff_time, limit),
            )
            return [
                {"text": row[0][:150] if row[0] else "", "published_at": row[1]}
                for row in cursor.fetchall()
            ]

    def get_published_embeddings(self, days: int = 60) -> list[tuple[int, np.ndarray]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å embeddings –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π

        Args:
            days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥

        Returns:
            –°–ø–∏—Å–æ–∫ (id, embedding)
        """
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()
            cutoff_time = now_utc() - timedelta(days=days)

            cursor.execute(
                """
                SELECT id, embedding FROM published
                WHERE published_at > ? AND embedding IS NOT NULL
            """,
                (cutoff_time,),
            )

            results = []
            for row in cursor.fetchall():
                buffer = io.BytesIO(row[1])
                embedding = np.load(buffer, allow_pickle=False)
                results.append((row[0], embedding))

            return results

    def check_duplicate(self, embedding: np.ndarray, threshold: float = 0.85, days: int = 60) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–º –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω–æ–≥–æ

        Args:
            embedding: Embedding —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0 - 1.0)
            days: –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 60 –¥–Ω–µ–π)

        Returns:
            True –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç
        """
        published_embeddings = self.get_published_embeddings(days=days)

        if not published_embeddings:
            return False

        embedding_norm = np.linalg.norm(embedding)
        if embedding_norm == 0:
            logger.warning("–ü–æ–ª—É—á–µ–Ω embedding —Å –Ω—É–ª–µ–≤–æ–π –Ω–æ—Ä–º–æ–π –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
            return False

        for post_id, published_embedding in published_embeddings:
            published_norm = np.linalg.norm(published_embedding)
            if published_norm == 0:
                logger.debug(
                    f"–ü—Ä–æ–ø—É—â–µ–Ω –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Å—Ç {post_id} –∏–∑-–∑–∞ –Ω—É–ª–µ–≤–æ–π –Ω–æ—Ä–º—ã embedding"
                )
                continue

            similarity = np.dot(embedding, published_embedding) / (embedding_norm * published_norm)

            if similarity >= threshold:
                logger.debug(f"–ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: post_id={post_id}, similarity={similarity:.3f}")
                return True

        return False

    # ====== –û–ß–ò–°–¢–ö–ê ======

    @retry_on_locked
    def cleanup_old_data(self, raw_days: int = 14, published_days: int = 60):
        """
        –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ

        Args:
            raw_days: –£–¥–∞–ª–∏—Ç—å raw_messages —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π
            published_days: –£–¥–∞–ª–∏—Ç—å published —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π
        """
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()

            raw_cutoff = now_utc() - timedelta(days=raw_days)
            published_cutoff = now_utc() - timedelta(days=published_days)

            conn.execute("BEGIN")
            try:
                cursor.execute("DELETE FROM raw_messages WHERE date < ?", (raw_cutoff,))
                raw_deleted = cursor.rowcount

                cursor.execute("DELETE FROM published WHERE published_at < ?", (published_cutoff,))
                published_deleted = cursor.rowcount

                conn.execute("COMMIT")
            except Exception:
                conn.execute("ROLLBACK")
                raise

            # VACUUM –¥–ª—è —Å–∂–∞—Ç–∏—è –ë–î (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–Ω–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏)
            cursor.execute("VACUUM")
            logger.info(
                f"–û—á–∏—Å—Ç–∫–∞ –ë–î: —É–¥–∞–ª–µ–Ω–æ {raw_deleted} —Å—ã—Ä—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π, "
                f"{published_deleted} –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤"
            )

            return {"raw": raw_deleted, "published": published_deleted}

    def get_stats(self) -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞–∑–µ"""
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()

            stats = {}

            cursor.execute("SELECT COUNT(*) FROM channels WHERE is_active = 1")
            stats["active_channels"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM raw_messages WHERE processed = 0")
            stats["unprocessed_messages"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM raw_messages")
            stats["total_messages"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM published")
            stats["total_published"] = cursor.fetchone()[0]

            return stats

    def get_today_stats(self, timezone_name: str | None = None) -> dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞ —Å–µ–≥–æ–¥–Ω—è

        Args:
            timezone_name: –ò–º—è timezone (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'Europe/Moscow').
                          –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è UTC.

        Returns:
            dict —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        with self._pool.get_connection() as conn:
            cursor = conn.cursor()
            stats = {}

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã "—Å–µ–≥–æ–¥–Ω—è" –≤ –Ω—É–∂–Ω–æ–π timezone
            if timezone_name:
                tz = get_timezone(timezone_name)
                now = now_in_timezone(tz)
                start_of_day = now.replace(hour=0, minute=0, second=0, microsecond=0)
                end_of_day = start_of_day + timedelta(days=1)
                start_utc = to_utc(start_of_day)
                end_utc = to_utc(end_of_day)
            else:
                from datetime import UTC

                now_utc_val = datetime.now(UTC)
                start_utc = now_utc_val.replace(hour=0, minute=0, second=0, microsecond=0)
                end_utc = start_utc + timedelta(days=1)

            start_str = start_utc.strftime("%Y-%m-%d %H:%M:%S")
            end_str = end_utc.strftime("%Y-%m-%d %H:%M:%S")

            cursor.execute(
                """
                SELECT COUNT(*) FROM raw_messages
                WHERE date >= ? AND date < ?
            """,
                (start_str, end_str),
            )
            stats["messages_today"] = cursor.fetchone()[0]

            cursor.execute(
                """
                SELECT COUNT(*) FROM raw_messages
                WHERE created_at >= ? AND created_at < ? AND processed = 1
            """,
                (start_str, end_str),
            )
            stats["processed_today"] = cursor.fetchone()[0]

            cursor.execute(
                """
                SELECT COUNT(*) FROM raw_messages
                WHERE processed = 0
            """
            )
            stats["unprocessed"] = cursor.fetchone()[0]

            cursor.execute(
                """
                SELECT COUNT(*) FROM published
                WHERE published_at >= ? AND published_at < ?
            """,
                (start_str, end_str),
            )
            stats["published_today"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM channels WHERE is_active = 1")
            stats["active_channels"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM raw_messages")
            stats["total_messages"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM published")
            stats["total_published"] = cursor.fetchone()[0]

            return stats

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î (idempotent, thread-safe)"""
        with self._lock:
            if self._closed:
                return
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º backward-compat —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
            if self._conn:
                try:
                    self._conn.close()
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")
                finally:
                    self._conn = None
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤–µ—Å—å –ø—É–ª
            try:
                self._pool.close_all()
                logger.info("Connection pool –∏ –ë–î –∑–∞–∫—Ä—ã—Ç—ã")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –ø—É–ª–∞: {e}")
            self._closed = True

    def __enter__(self):
        """Context manager support"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager cleanup"""
        self.close()
        return False

    def __del__(self):
        """Cleanup on garbage collection"""
        try:
            self.close()
        except Exception:
            pass
