#!/usr/bin/env python3
"""
ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞºĞ°Ğ½ĞµÑ€ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼.

ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ: Telegram ÑÑ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ¡Ğ£ĞœĞœĞĞ ĞĞ«Ğ™ Ğ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° ÑĞµÑÑĞ¸Ñ,
Ğ° Ğ½Ğµ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ·Ğ° Ğ¿Ğ°Ñ‡ĞºÑƒ. ĞŸĞ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ°Ñ‡ĞºĞ¸,
Ğ½Ğ¾ Ğ¸ Ğ½Ğ° ĞºĞ°ĞºĞ¾Ğ¼ Ğ¡Ğ£ĞœĞœĞĞ ĞĞĞœ ĞºĞ°Ğ½Ğ°Ğ»Ğµ Ğ·Ğ° Ğ´ĞµĞ½ÑŒ ÑĞ»ÑƒÑ‡Ğ¸Ğ»ÑÑ FloodWait.

ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ:
  - ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ FloodWait Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼: ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğ¹ ĞºĞ°Ğ½Ğ°Ğ» Ğ·Ğ° Ğ´ĞµĞ½ÑŒ,
    Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ¿Ğ°Ñ‡ĞºĞ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°Ñ‡ĞºĞ¸, Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ°, Ğ²Ñ€ĞµĞ¼Ñ ÑÑƒÑ‚Ğ¾Ğº
  - Ğ˜Ğ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ÑÑ "Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ Ğ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚" (safe_daily_limit)
  - ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ°Ñ‡ĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ Ñ‚Ğ°Ğº Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ‚ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ·Ğ° Ğ´ĞµĞ½ÑŒ
  - ĞŸĞ¾ÑĞ»Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ¾Ğ² Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ
  - Entity ĞºÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ¸ÑĞº â€” Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¸ Ğ½Ğµ Ğ´ĞµĞ»Ğ°ÑÑ‚ ResolveUsernameRequest

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    python scripts/scan_channel_stats.py --profile marketplace
    python scripts/scan_channel_stats.py --profile ai
"""

import argparse
import asyncio
import json
import os
import re
import sys
from datetime import datetime, timedelta
from pathlib import Path

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from telethon import TelegramClient
from telethon.errors import FloodWaitError, ChannelPrivateError, ChatAdminRequiredError
from telethon.tl.functions.channels import GetFullChannelRequest
from telethon.tl.types import InputChannel

from database.db import Database
from utils.config import load_config
from utils.logger import setup_logger, configure_logging
from utils.telegram_helpers import safe_connect

logger = setup_logger(__name__)

LEARNING_DIR = Path(__file__).parent.parent / "data" / "scan_learning"

# ĞĞ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
MIN_BATCH_SIZE    = 3
MAX_BATCH_SIZE    = 50
MIN_BATCH_PAUSE   = 120    # 2 Ğ¼Ğ¸Ğ½
MAX_BATCH_PAUSE   = 7200   # 2 Ñ‡Ğ°ÑĞ°
MIN_CHANNEL_DELAY = 15
MAX_CHANNEL_DELAY = 180
MIN_DAILY_LIMIT   = 10     # Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ½Ğµ Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞ¼Ñ‹ÑĞ»Ğ°
MAX_DAILY_LIMIT   = 500    # Ğ¿Ğ¾Ñ‚Ğ¾Ğ»Ğ¾Ğº


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ScanKnowledge â€” Ğ¼Ğ¾Ğ·Ğ³ ÑĞºĞ°Ğ½ĞµÑ€Ğ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ScanKnowledge:
    """
    ĞŸĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ÑĞºĞ°Ğ½ĞµÑ€Ğ°. Ğ¥Ñ€Ğ°Ğ½Ğ¸Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ FloodWait Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ¹.

    Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°: safe_daily_limit â€” ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¾ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
    Ğ·Ğ° Ğ´ĞµĞ½ÑŒ Ğ±ĞµĞ· Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ÑÑ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸:
      - Ğ•ÑĞ»Ğ¸ Ñ„Ğ»ÑƒĞ´ ÑĞ»ÑƒÑ‡Ğ¸Ğ»ÑÑ Ğ½Ğ° ĞºĞ°Ğ½Ğ°Ğ»Ğµ N â€” Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ < N
      - safe_daily_limit = min(Ğ²ÑĞµ N Ğ³Ğ´Ğµ Ğ±Ñ‹Ğ» Ñ„Ğ»ÑƒĞ´) * safety_factor
      - ĞŸĞ¾ÑĞ»Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ¾Ğ² â€” Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµĞ¼

    Ğ˜Ğ· safe_daily_limit Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑÑ‚ÑÑ batch_size Ğ¸ batch_pause:
      - Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´ĞµĞ½ÑŒ (86400Ñ) Ğ½Ğ° Ñ€Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ¾ĞºĞ½Ğ°
      - batch_size = safe_daily_limit / num_batches_per_day
      - batch_pause = 86400 / num_batches_per_day - batch_size * channel_delay
    """

    def __init__(self, profile: str):
        LEARNING_DIR.mkdir(parents=True, exist_ok=True)
        self.path = LEARNING_DIR / f"{profile}.json"
        self.profile = profile
        self._load()

    def _load(self):
        if self.path.exists():
            try:
                data = json.loads(self.path.read_text())
            except Exception:
                data = {}
        else:
            data = {}

        # Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ â€” Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ Ğ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚
        self.safe_daily_limit = data.get("safe_daily_limit", 80)
        self.channel_delay    = data.get("channel_delay", 45.0)

        # Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹
        self.flood_history    = data.get("flood_history", [])    # FloodWait ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ
        self.run_history      = data.get("run_history", [])      # Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ñ‹
        self.success_runs     = data.get("success_runs", 0)      # Ğ¿Ğ¾Ğ´Ñ€ÑĞ´ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ¾Ğ²
        self.total_floods     = data.get("total_floods", 0)

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ°Ñ‡ĞµĞº Ğ¸Ğ· safe_daily_limit
        self._recalc_batch_params()

    def _recalc_batch_params(self):
        """
        Ğ˜Ğ· safe_daily_limit Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ batch_size Ğ¸ batch_pause.
        Ğ›Ğ¾Ğ³Ğ¸ĞºĞ°: Ğ´ĞµĞ»Ğ¸Ğ¼ Ğ´ĞµĞ½ÑŒ Ğ½Ğ° Ğ¾ĞºĞ½Ğ°, Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ¾ĞºĞ½Ğµ â€” Ğ¾Ğ´Ğ½Ğ° Ğ¿Ğ°Ñ‡ĞºĞ°.
        ĞŸĞ°Ñ‡ĞºĞ° Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ 30% Ğ¾ĞºĞ½Ğ° (Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ â€” Ğ¿Ğ°ÑƒĞ·Ğ°).
        """
        # Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ°Ñ‡ĞµĞº Ğ² Ğ´ĞµĞ½ÑŒ Ğ½Ğ°Ğ¼ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚ÑŒ safe_daily_limit
        # Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ğ¾Ğ¼ batch_size
        target_batch = max(MIN_BATCH_SIZE, min(20, self.safe_daily_limit // 5))
        num_batches = max(1, (self.safe_daily_limit + target_batch - 1) // target_batch)

        # Ğ’Ñ€ĞµĞ¼Ñ Ğ¾ĞºĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ‡ĞºĞ¸
        window_s = 86400 // num_batches

        # Ğ’Ñ€ĞµĞ¼Ñ Ğ½Ğ° ÑĞ°Ğ¼Ñƒ Ğ¿Ğ°Ñ‡ĞºÑƒ (ĞºĞ°Ğ½Ğ°Ğ»Ñ‹ * Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ°)
        scan_time = target_batch * self.channel_delay

        # ĞŸĞ°ÑƒĞ·Ğ° = Ğ¾ĞºĞ½Ğ¾ - Ğ²Ñ€ĞµĞ¼Ñ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ MIN_BATCH_PAUSE
        pause = max(MIN_BATCH_PAUSE, int(window_s - scan_time))
        pause = min(MAX_BATCH_PAUSE, pause)

        self.batch_size  = target_batch
        self.batch_pause = pause

    def save(self):
        data = {
            "safe_daily_limit": self.safe_daily_limit,
            "channel_delay":    round(self.channel_delay, 1),
            "batch_size":       self.batch_size,
            "batch_pause":      self.batch_pause,
            "flood_history":    self.flood_history[-100:],
            "run_history":      self.run_history[-30:],
            "success_runs":     self.success_runs,
            "total_floods":     self.total_floods,
            "updated_at":       datetime.utcnow().isoformat(),
        }
        self.path.write_text(json.dumps(data, indent=2, ensure_ascii=False))

    # â”€â”€ Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def on_flood(
        self,
        wait_seconds: int,
        *,
        total_scanned_today: int,   # Ğ¡ĞšĞĞ›Ğ¬ĞšĞ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¾ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ Ğ´Ğ¾ Ñ„Ğ»ÑƒĞ´Ğ°
        batch_num: int,             # Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ¿Ğ°Ñ‡ĞºĞ¸ (1, 2, 3...)
        batch_size: int,            # Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°Ñ‡ĞºĞ¸ Ğ² Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ñ„Ğ»ÑƒĞ´Ğ°
        channel_in_batch: int,      # Ğ½Ğ° ĞºĞ°ĞºĞ¾Ğ¼ ĞºĞ°Ğ½Ğ°Ğ»Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ‡ĞºĞ¸ ÑĞ»ÑƒÑ‡Ğ¸Ğ»ÑÑ Ñ„Ğ»ÑƒĞ´
        channel_delay: float,
    ):
        """
        Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ FloodWait.

        total_scanned_today â€” ÑÑ‚Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°:
        Ñ„Ğ»ÑƒĞ´ ÑĞ»ÑƒÑ‡Ğ¸Ğ»ÑÑ ĞŸĞĞ¡Ğ›Ğ• Ñ‚Ğ¾Ğ³Ğ¾ ĞºĞ°Ğº Ğ¼Ñ‹ Ğ¿Ñ€Ğ¾ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ total_scanned_today ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ².
        Ğ—Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ ÑĞµÑÑĞ¸Ğ¸ < total_scanned_today.
        """
        self.total_floods += 1
        self.success_runs = 0  # ÑĞ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ ÑÑ‡Ñ‘Ñ‚Ñ‡Ğ¸Ğº ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ¾Ğ²

        # Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğ¹ ĞºĞ°Ğ½Ğ°Ğ» Ğ·Ğ° Ğ´ĞµĞ½ÑŒ Ğ³Ğ´Ğµ ÑĞ»ÑƒÑ‡Ğ¸Ğ»ÑÑ Ñ„Ğ»ÑƒĞ´
        flood_at_channel = total_scanned_today + channel_in_batch

        event = {
            "ts":                datetime.utcnow().isoformat(),
            "wait_s":            wait_seconds,
            "flood_at_channel":  flood_at_channel,   # â† Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°
            "batch_num":         batch_num,
            "batch_size":        batch_size,
            "channel_in_batch":  channel_in_batch,
            "channel_delay":     round(channel_delay, 1),
            "old_daily_limit":   self.safe_daily_limit,
        }
        self.flood_history.append(event)

        # â”€â”€ Ğ£Ñ€Ğ¾Ğº 1: Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ safe_daily_limit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        old_limit = self.safe_daily_limit

        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ "Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ñ„Ğ»ÑƒĞ´Ğ°" Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸
        flood_points = [e["flood_at_channel"] for e in self.flood_history
                        if "flood_at_channel" in e]

        if flood_points:
            # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ° Ñ„Ğ»ÑƒĞ´Ğ° â€” ÑÑ‚Ğ¾ Ğ²ĞµÑ€Ñ…Ğ½ÑÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ°
            min_flood_point = min(flood_points)

            # safety_factor Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ‚ÑĞ¶ĞµÑÑ‚Ğ¸ Ñ„Ğ»ÑƒĞ´Ğ°:
            # Ñ‚ÑĞ¶Ñ‘Ğ»Ñ‹Ğ¹ Ñ„Ğ»ÑƒĞ´ â†’ Ğ±ĞµÑ€Ñ‘Ğ¼ 50% Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ñ„Ğ»ÑƒĞ´Ğ°
            # Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ â†’ 70%
            if wait_seconds > 3600:
                safety_factor = 0.50
            elif wait_seconds > 300:
                safety_factor = 0.60
            else:
                safety_factor = 0.70

            new_limit = max(MIN_DAILY_LIMIT, int(min_flood_point * safety_factor))
            # Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ‚ÑŒÑÑ Ğ¿Ñ€Ğ¸ Ñ„Ğ»ÑƒĞ´Ğµ, Ğ½Ğ¸ĞºĞ¾Ğ³Ğ´Ğ° Ğ½Ğµ Ñ€Ğ°ÑÑ‚Ğ¸
            self.safe_daily_limit = min(self.safe_daily_limit, new_limit)
        else:
            # ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ğ»ÑƒĞ´ â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚
            if wait_seconds > 3600:
                self.safe_daily_limit = max(MIN_DAILY_LIMIT, int(self.safe_daily_limit * 0.5))
            elif wait_seconds > 300:
                self.safe_daily_limit = max(MIN_DAILY_LIMIT, int(self.safe_daily_limit * 0.6))
            else:
                self.safe_daily_limit = max(MIN_DAILY_LIMIT, int(self.safe_daily_limit * 0.75))

        # â”€â”€ Ğ£Ñ€Ğ¾Ğº 2: Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ channel_delay â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if wait_seconds > 3600:
            self.channel_delay = min(MAX_CHANNEL_DELAY, self.channel_delay * 1.5)
        elif wait_seconds > 300:
            self.channel_delay = min(MAX_CHANNEL_DELAY, self.channel_delay * 1.3)
        else:
            self.channel_delay = min(MAX_CHANNEL_DELAY, self.channel_delay * 1.1)

        # ĞŸĞµÑ€ĞµÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ°Ñ‡ĞµĞº
        self._recalc_batch_params()

        severity = "ğŸ”´ Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ğ¹ (>1Ñ‡)" if wait_seconds > 3600 else \
                   "ğŸŸ¡ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğ¹ (>5Ğ¼Ğ¸Ğ½)" if wait_seconds > 300 else \
                   "ğŸŸ¢ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ (<5Ğ¼Ğ¸Ğ½)"

        logger.warning(
            "ğŸ§  Ğ£Ñ€Ğ¾Ğº #%d [%s] FloodWait=%ds Ğ½Ğ° ĞºĞ°Ğ½Ğ°Ğ»Ğµ %d Ğ·Ğ° Ğ´ĞµĞ½ÑŒ\n"
            "   safe_daily_limit: %d â†’ %d | delay: %.0fs â†’ %.0fs\n"
            "   ĞĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: batch=%d, pause=%ds (%dĞ¼Ğ¸Ğ½)",
            self.total_floods, severity, wait_seconds, flood_at_channel,
            old_limit, self.safe_daily_limit,
            channel_delay, self.channel_delay,
            self.batch_size, self.batch_pause, self.batch_pause // 60,
        )
        self.save()

    def on_run_complete(self, total_scanned: int, total_channels: int, had_flood: bool):
        """
        Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ°.
        Ğ•ÑĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½ Ğ¿Ñ€Ğ¾ÑˆÑ‘Ğ» Ğ±ĞµĞ· Ñ„Ğ»ÑƒĞ´Ğ° â€” Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚.
        """
        event = {
            "ts":            datetime.utcnow().isoformat(),
            "total_scanned": total_scanned,
            "total_channels": total_channels,
            "had_flood":     had_flood,
            "daily_limit":   self.safe_daily_limit,
        }
        self.run_history.append(event)

        if not had_flood and total_scanned >= total_channels:
            # ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½ Ğ±ĞµĞ· Ñ„Ğ»ÑƒĞ´Ğ°
            self.success_runs += 1

            # ĞŸĞ¾ÑĞ»Ğµ 2 ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ¾Ğ´Ñ€ÑĞ´ â€” Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµĞ¼ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ½Ğ° 10%
            if self.success_runs >= 2:
                old = self.safe_daily_limit
                self.safe_daily_limit = min(
                    MAX_DAILY_LIMIT,
                    int(self.safe_daily_limit * 1.10)
                )
                self.channel_delay = max(
                    MIN_CHANNEL_DELAY,
                    self.channel_delay * 0.95
                )
                self._recalc_batch_params()

                if self.safe_daily_limit != old:
                    logger.info(
                        "ğŸ§  ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (%d ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ¾Ğ²): "
                        "daily_limit %dâ†’%d, delay %.0fsâ†’%.0fs, batch=%d, pause=%ds",
                        self.success_runs, old, self.safe_daily_limit,
                        self.channel_delay / 0.95, self.channel_delay,
                        self.batch_size, self.batch_pause,
                    )
        else:
            self.success_runs = 0

        self.save()

    def summary(self) -> str:
        """ĞšÑ€Ğ°Ñ‚ĞºĞ°Ñ ÑĞ²Ğ¾Ğ´ĞºĞ° Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."""
        return (
            f"daily_limit={self.safe_daily_limit} | "
            f"batch={self.batch_size} ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² | "
            f"pause={self.batch_pause}Ñ ({self.batch_pause//60}Ğ¼Ğ¸Ğ½) | "
            f"delay={self.channel_delay:.0f}Ñ | "
            f"Ñ„Ğ»ÑƒĞ´Ğ¾Ğ² Ğ²ÑĞµĞ³Ğ¾: {self.total_floods} | "
            f"ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ¾Ğ´Ñ€ÑĞ´: {self.success_runs}"
        )

    def flood_analysis(self) -> str:
        """ĞĞ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ„Ğ»ÑƒĞ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸."""
        if not self.flood_history:
            return "Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ„Ğ»ÑƒĞ´Ğ¾Ğ² Ğ¿ÑƒÑÑ‚Ğ°"
        points = [e["flood_at_channel"] for e in self.flood_history if "flood_at_channel" in e]
        if not points:
            return "ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ‚Ğ¾Ñ‡ĞºĞ°Ñ… Ñ„Ğ»ÑƒĞ´Ğ°"
        return (
            f"Ğ¢Ğ¾Ñ‡ĞºĞ¸ Ñ„Ğ»ÑƒĞ´Ğ°: min={min(points)}, max={max(points)}, "
            f"avg={sum(points)//len(points)} | "
            f"ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ: {points[-5:]}"
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EntityCache â€” ĞºÑÑˆ TG entity
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EntityCache:
    """
    ĞšÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ channel_id + access_hash Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ ResolveUsernameRequest.
    Ğ­Ñ‚Ğ¾ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº TG API.
    """

    def __init__(self, profile: str):
        LEARNING_DIR.mkdir(parents=True, exist_ok=True)
        self.path = LEARNING_DIR / f"{profile}_entity_cache.json"
        self._cache: dict[str, dict] = {}
        self._load()

    def _load(self):
        if self.path.exists():
            try:
                self._cache = json.loads(self.path.read_text())
            except Exception:
                self._cache = {}

    def save(self):
        self.path.write_text(json.dumps(self._cache, indent=2))

    def get(self, username: str) -> InputChannel | None:
        entry = self._cache.get(username.lower())
        if entry:
            return InputChannel(entry["channel_id"], entry["access_hash"])
        return None

    def put(self, username: str, entity):
        try:
            self._cache[username.lower()] = {
                "channel_id":  entity.id,
                "access_hash": entity.access_hash,
                "title":       getattr(entity, "title", ""),
                "cached_at":   datetime.utcnow().isoformat(),
            }
        except AttributeError:
            pass


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ’ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_contacts(text: str) -> str:
    if not text:
        return ""
    found = set()
    found.update(re.findall(r"@[\w]{3,}", text))
    found.update(re.findall(r"https?://[^\s]+", text))
    found.update(re.findall(r"[\w.+-]+@[\w-]+\.[a-zA-Z]{2,}", text))
    return ", ".join(sorted(found))


async def ensure_connected(client: TelegramClient, session_name: str) -> None:
    if not client.is_connected():
        logger.info("  ğŸ”„ ĞŸĞµÑ€ĞµĞ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº Telegram...")
        await safe_connect(client, session_name)


def get_already_scanned_today(db: Database) -> set[int]:
    cutoff = datetime.utcnow() - timedelta(hours=25)
    with db._pool.get_connection() as conn:
        rows = conn.execute(
            "SELECT DISTINCT channel_id FROM channel_stats WHERE scanned_at >= ?",
            (cutoff,),
        ).fetchall()
    return {row[0] for row in rows}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ½Ğ°Ğ»Ğ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FloodSignal(Exception):
    """Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»: Ğ¿Ğ¾Ğ¹Ğ¼Ğ°Ğ»Ğ¸ FloodWait, Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²ĞµÑ€Ñ…Ñƒ."""
    def __init__(self, seconds: int):
        self.seconds = seconds


async def scan_one(
    client: TelegramClient,
    session_name: str,
    db: Database,
    channel: dict,
    delay: float,
    entity_cache: EntityCache,
) -> bool:
    """
    Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ´Ğ¸Ğ½ ĞºĞ°Ğ½Ğ°Ğ».
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ True ĞµÑĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½, False ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶ĞµĞ½ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ (Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ).
    Ğ‘Ñ€Ğ¾ÑĞ°ĞµÑ‚ FloodSignal ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ğ¹Ğ¼Ğ°Ğ»Ğ¸ FloodWait.
    """
    username = channel["username"]
    channel_id = channel["id"]

    await ensure_connected(client, session_name)

    # Ğ‘ĞµÑ€Ñ‘Ğ¼ entity Ğ¸Ğ· ĞºÑÑˆĞ° ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ â€” ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¼ ResolveUsernameRequest
    cached = entity_cache.get(username)
    if cached:
        try:
            entity = await client.get_entity(cached)
        except Exception:
            entity_cache._cache.pop(username.lower(), None)
            entity = await client.get_entity(username)
    else:
        entity = await client.get_entity(username)

    entity_cache.put(username, entity)

    full = await client(GetFullChannelRequest(entity))
    fc = full.full_chat

    participants = getattr(fc, "participants_count", 0) or 0
    about = getattr(fc, "about", "") or ""
    contacts = extract_contacts(about)

    views_list = []
    async for msg in client.iter_messages(entity, limit=20):
        if msg.views:
            views_list.append(msg.views)
        with db._pool.get_connection() as conn:
            conn.execute(
                "UPDATE raw_messages SET views=?, forwards=? WHERE channel_id=? AND message_id=?",
                (msg.views or 0, msg.forwards or 0, channel_id, msg.id),
            )
    avg_views = int(sum(views_list) / len(views_list)) if views_list else 0

    db.update_channel_stats(channel_id, participants, avg_views, about, contacts)
    logger.info(
        "  âœ… @%-30s  %7d Ğ¿Ğ¾Ğ´Ğ¿.  avg %5d Ğ¿Ñ€Ğ¾ÑĞ¼.%s",
        username, participants, avg_views,
        f"  ğŸ“¬ {contacts}" if contacts else "",
    )
    await asyncio.sleep(delay)
    return True


async def scan_channel_safe(
    client: TelegramClient,
    session_name: str,
    db: Database,
    channel: dict,
    delay: float,
    entity_cache: EntityCache,
) -> bool:
    """
    ĞĞ±Ñ‘Ñ€Ñ‚ĞºĞ° Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹.
    Ğ‘Ñ€Ğ¾ÑĞ°ĞµÑ‚ FloodSignal Ğ¿Ñ€Ğ¸ FloodWait.
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ True/False (done/retry).
    """
    try:
        return await scan_one(client, session_name, db, channel, delay, entity_cache)

    except FloodWaitError as e:
        raise FloodSignal(e.seconds)

    except (ChannelPrivateError, ChatAdminRequiredError):
        logger.debug("  âš ï¸  @%s Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½", channel["username"])
        await asyncio.sleep(min(delay, 10))
        return True

    except Exception as e:
        err = str(e)
        if "disconnected" in err.lower():
            logger.warning("  ğŸ”Œ @%s: Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ, Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ...", channel["username"])
            try:
                await safe_connect(client, session_name)
            except Exception as ce:
                logger.error("  ĞŸĞµÑ€ĞµĞ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ: %s", ce)
                await asyncio.sleep(10)
            return False  # Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ÑŒ
        logger.warning("  âŒ @%s: %s", channel["username"], e)
        await asyncio.sleep(min(delay, 10))
        return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ»
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main(profile: str):
    config = load_config(profile=profile)
    configure_logging(
        level=config.log_level,
        log_file=config.log_file,
        rotation=config.log_rotation,
        file_format=config.log_format,
        date_format=config.log_date_format,
    )

    knowledge = ScanKnowledge(profile)
    entity_cache = EntityCache(profile)

    db = Database(config.db_path, **config.database_settings())
    all_channels = db.get_active_channels()
    total = len(all_channels)

    if total == 0:
        logger.error("ĞĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğµ %s", profile)
        sys.exit(1)

    already_done = get_already_scanned_today(db)
    channels_todo = [ch for ch in all_channels if ch["id"] not in already_done]
    skipped_resume = total - len(channels_todo)

    logger.info("=" * 72)
    logger.info("ğŸ“¡ ĞĞ”ĞĞŸĞ¢Ğ˜Ğ’ĞĞ«Ğ™ Ğ¡ĞšĞĞĞ•Ğ  â€” Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ: %s", profile)
    logger.info("   Ğ’ÑĞµĞ³Ğ¾ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²: %d  |  Ğš ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: %d  |  Ğ£Ğ¶Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾: %d",
                total, len(channels_todo), skipped_resume)
    logger.info("   ğŸ§  %s", knowledge.summary())
    logger.info("   ğŸ“Š %s", knowledge.flood_analysis())
    logger.info("   ğŸ“¦ Entity Ğ² ĞºÑÑˆĞµ: %d", len(entity_cache._cache))
    logger.info("=" * 72)

    if not channels_todo:
        logger.info("âœ… Ğ’ÑĞµ ĞºĞ°Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¾ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹.")
        db.close()
        return

    # Ğ¡ĞµÑÑĞ¸Ñ (Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ‚ listener)
    base_session = config.get("telegram.session_name", "")
    if base_session.endswith("/session"):
        session_name = base_session[:-8] + "/processor"
    elif base_session.endswith("/session.session"):
        session_name = base_session[:-16] + "/processor"
    else:
        session_name = base_session + "_processor"

    client = TelegramClient(
        session_name,
        config.telegram_api_id,
        config.telegram_api_hash,
        flood_sleep_threshold=60,  # FloodWait > 60Ñ â€” Ğ±Ñ€Ğ¾ÑĞ°ĞµĞ¼ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ, Ğ½Ğµ ÑĞ¿Ğ¸Ğ¼ ÑĞ°Ğ¼Ğ¸
    )

    had_flood_this_run = False

    try:
        await safe_connect(client, session_name)

        remaining = list(channels_todo)
        scanned_today = skipped_resume   # ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¾ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ·Ğ° Ğ´ĞµĞ½ÑŒ (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ resume)
        scanned_this_run = 0
        with_contacts = 0
        batch_num = 0

        while remaining:
            batch_num += 1
            batch = remaining[:knowledge.batch_size]

            logger.info(
                "â”€â”€ ĞŸĞ°Ñ‡ĞºĞ° #%d: %d ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² | delay=%.0fÑ | Ğ¿Ğ°ÑƒĞ·Ğ° Ğ¿Ğ¾ÑĞ»Ğµ=%ds (%dĞ¼Ğ¸Ğ½) "
                "| Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° Ğ´ĞµĞ½ÑŒ: %d/%d â”€â”€",
                batch_num, len(batch),
                knowledge.channel_delay,
                knowledge.batch_pause, knowledge.batch_pause // 60,
                scanned_today, knowledge.safe_daily_limit,
            )

            batch_scanned = 0
            flood_in_batch = False

            for ch_in_batch, channel in enumerate(batch, start=1):
                idx_global = skipped_resume + scanned_this_run + 1
                logger.info("[%d/%d] @%s  (Ğ´ĞµĞ½ÑŒ: %d/%d)",
                            idx_global, total, channel["username"],
                            scanned_today + 1, knowledge.safe_daily_limit)

                # ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµĞ¼ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ
                while True:
                    try:
                        done = await scan_channel_safe(
                            client, session_name, db, channel,
                            knowledge.channel_delay, entity_cache,
                        )
                    except FloodSignal as fs:
                        # â”€â”€ ĞŸĞ¾Ğ¹Ğ¼Ğ°Ğ»Ğ¸ FloodWait â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                        had_flood_this_run = True
                        flood_in_batch = True

                        knowledge.on_flood(
                            fs.seconds,
                            total_scanned_today=scanned_today,  # â† ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ğ·Ğ° Ğ´ĞµĞ½ÑŒ
                            batch_num=batch_num,
                            batch_size=len(batch),
                            channel_in_batch=ch_in_batch,       # â† Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ‡ĞºĞ¸
                            channel_delay=knowledge.channel_delay,
                        )

                        wait = fs.seconds + 10
                        logger.warning(
                            "  â³ FloodWait %ds Ğ´Ğ»Ñ @%s â€” Ğ¶Ğ´Ñƒ %ds...",
                            fs.seconds, channel["username"], wait,
                        )
                        await asyncio.sleep(wait)
                        break  # Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ‡ĞºÑƒ, Ğ¸Ğ´Ñ‘Ğ¼ Ğ½Ğ° Ğ¿Ğ°ÑƒĞ·Ñƒ

                    if not done:
                        continue  # Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ÑŒ ĞºĞ°Ğ½Ğ°Ğ» (Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ)

                    # Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾
                    remaining.pop(0)
                    batch_scanned += 1
                    scanned_today += 1
                    scanned_this_run += 1

                    try:
                        with db._pool.get_connection() as conn:
                            row = conn.execute(
                                "SELECT contact_info FROM channel_stats "
                                "WHERE channel_id=? ORDER BY scanned_at DESC LIMIT 1",
                                (channel["id"],),
                            ).fetchone()
                        if row and row[0]:
                            with_contacts += 1
                    except Exception:
                        pass
                    break  # ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ğ½Ğ°Ğ» Ğ² Ğ¿Ğ°Ñ‡ĞºĞµ

                if flood_in_batch:
                    break  # Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ for-loop Ğ¿Ğ°Ñ‡ĞºĞ¸

            # â”€â”€ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ĞºÑÑˆ Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ‡ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            entity_cache.save()

            # â”€â”€ ĞŸĞ°ÑƒĞ·Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ‡ĞºĞ°Ğ¼Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if not remaining:
                break

            if flood_in_batch:
                # ĞŸĞ¾ÑĞ»Ğµ Ñ„Ğ»ÑƒĞ´Ğ°: ÑƒĞ¶Ğµ Ğ¶Ğ´Ğ°Ğ»Ğ¸ flood_wait, Ñ‚ĞµĞ¿ĞµÑ€ÑŒ ĞµÑ‰Ñ‘ batch_pause
                extra = knowledge.batch_pause
                logger.info(
                    "â¸  ĞŸĞ°ÑƒĞ·Ğ° Ğ¿Ğ¾ÑĞ»Ğµ FloodWait: %ds (%dĞ¼Ğ¸Ğ½) | ğŸ§  %s",
                    extra, extra // 60, knowledge.summary(),
                )
                await asyncio.sleep(extra)
            else:
                # Ğ£ÑĞ¿ĞµÑˆĞ½Ğ°Ñ Ğ¿Ğ°Ñ‡ĞºĞ°
                pause = knowledge.batch_pause
                logger.info(
                    "â¸  ĞŸĞ°ÑƒĞ·Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ‡ĞºĞ°Ğ¼Ğ¸: %ds (%dĞ¼Ğ¸Ğ½) | "
                    "Ğ´ĞµĞ½ÑŒ: %d/%d Ğ¿Ñ€Ğ¾ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ | ğŸ§  %s",
                    pause, pause // 60,
                    scanned_today, knowledge.safe_daily_limit,
                    knowledge.summary(),
                )
                await asyncio.sleep(pause)

        # â”€â”€ Ğ¤Ğ¸Ğ½Ğ°Ğ» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        entity_cache.save()
        knowledge.on_run_complete(scanned_this_run, len(channels_todo), had_flood_this_run)

        logger.info("=" * 72)
        logger.info("âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾: %d Ğ½Ğ¾Ğ²Ñ‹Ñ… + %d Ğ¸Ğ· ĞºÑÑˆĞ° | ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹: %d ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²",
                    scanned_this_run, skipped_resume, with_contacts)
        logger.info("ğŸ§  Ğ˜Ñ‚Ğ¾Ğ³: %s", knowledge.summary())
        logger.info("ğŸ“Š %s", knowledge.flood_analysis())

        # Ğ¢Ğ¾Ğ¿-10
        try:
            with db._pool.get_connection() as conn:
                rows = conn.execute(
                    """SELECT c.username, cs.participants_count, cs.avg_message_views
                       FROM channel_stats cs JOIN channels c ON cs.channel_id = c.id
                       WHERE cs.id IN (SELECT MAX(id) FROM channel_stats GROUP BY channel_id)
                       ORDER BY cs.participants_count DESC LIMIT 10"""
                ).fetchall()
            logger.info("=" * 72)
            logger.info("ğŸ“Š Ğ¢ĞĞŸ-10 Ğ¿Ğ¾ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑ‡Ğ¸ĞºĞ°Ğ¼:")
            for i, (uname, subs, avg) in enumerate(rows, 1):
                logger.info("  %2d. @%-30s  %7d Ğ¿Ğ¾Ğ´Ğ¿.  avg %5d Ğ¿Ñ€Ğ¾ÑĞ¼.", i, uname, subs, avg)
        except Exception as e:
            logger.warning("ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ¿: %s", e)

    finally:
        await client.disconnect()
        db.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞºĞ°Ğ½ĞµÑ€ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Telegram-ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²")
    parser.add_argument("--profile", required=True, choices=["ai", "marketplace"])
    args = parser.parse_args()
    os.environ["PROFILE"] = args.profile
    asyncio.run(main(args.profile))
